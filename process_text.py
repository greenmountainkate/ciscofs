# coding=utf-8
"""
This module processes the parsed file object generated by the BeautifulSoup
library.

Functions include:

get_headers(), clean_text(), clean_text_remove_headers(), clear_stop_words(), and
process_imp_words()

"""
import string
import operator
import math
import numpy as np
from nltk.corpus import stopwords
from db_updates import *


def get_headers(parsed_file_object):
    """
    Locates all headers of types h1 - h5 and header, pulls text from those tags, and\
    generates a list of that header text.  No cleaning is done on header text.  Whitespace and punctuation
    remain for readability
    
    :param parsed_file_object: Object formed by parsing an html file with BeautifulSoup4.  Python tree structure.
    :return:  list of headers pulled from the html body of the parsed file
    """
    return [header.get_text() for header in parsed_file_object.body.find_all(['h1', 'h2', 'h3', 'h4', 'h5'])]


def clean_text(pfo):
    """
    Processes all text within the html body.  Removes script tags and text.  Removes
    extraneous whitespace and punctuation from body text and adds to a list of words.

    :param pfo: Object formed by parsing an html file with BeautifulSoup4.  Python tree structure.
    :rtype: list of strings, where each string is a single word pulled from html body, including
    header text
    """
    for scpt in pfo.body.find_all(['script']):
        scpt.decompose()  # remove scripts
    return [word for text in pfo.body.stripped_strings for word in
            (repr(text)).translate(str.maketrans('', '', string.punctuation)).lower().split()]


def clean_text_remove_headers(pfo):
    """
    Processes all text within the html body.  Removes script tags/text and all header tags/texts.  Removes
    extraneous whitespace and punctuation from body text and adds to a list of words.

    :param pfo: Object formed by parsing an html file with BeautifulSoup4.  Python tree structure.
    :rtype: list of strings, where each string is a single word pulled from html body
    """
    for header in pfo.body.find_all(['h1', 'h2', 'h3', 'h4', 'h5']):
        header.decompose()
    return clean_text(pfo)


def clear_stop_words(word_list):
    """
    Uses dictionary of common English words from nltk library to clear repetitive function words from
    html text.  Examples of removed words include 'a', 'an', 'the', etc.  This implementation
    assumes primarily English text sites will be processed and would need to be updated for use in other
    language website processing.
    
    :param word_list: list of strings, where each item is a single word string
    :return: list of strings, each a single word, with common function words removed
    """
    stop = set(stopwords.words('english'))
    return [word for word in word_list if word not in stop]


def process_imp_words(pf_db, error_db, file_id, word_list):
    """
    Identifies important words from a file's cleaned word list.  Important words are arbitrarily defined as the most
    frequent words in the document, with the number of words limited to the minimum of 20 or 20% of total words in
    the words, to optimize storage and runtime. If time permitted, it would be ideal to run tf-idf analysis on the
    file text and update the important words accordingly.

    :param error_db: Mongo database collection to receive error logging
    :param pf_db: Mongo database collection that holds parsed file documents
    :param file_id: unique identifier for the html file document in the parsed file document collection
    :param word_list: list of strings, expects single word strings with common/functional words previously removed
    """
    word_array = np.array(word_list)
    unique, counts = np.unique(word_array, return_counts=True)
    # zip unique/counts then sort by counts
    keyword_list = sorted(zip(unique, counts), key=operator.itemgetter(1), reverse=True)

    # reduce number of important words on large files, keep all words up to 20% min > 20 words
    num = math.floor(len(unique) * 0.2)  # calc ~20% of word list (arbitrary value)
    if num > 20:
        keyword_list = keyword_list[0:num]

    # add keyword list to this file's doc and update all previous doc lists that contain any keyword on list
    for x in keyword_list:
        upsert_keyword_and_update_lists(pf_db=pf_db, error_db=error_db, doc_id=file_id, tup=x)
